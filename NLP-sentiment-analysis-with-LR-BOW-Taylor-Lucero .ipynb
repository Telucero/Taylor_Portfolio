{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#About-Train,-validation-and-test-sets\" data-toc-modified-id=\"About-Train,-validation-and-test-sets-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><a href=\"https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\" target=\"_blank\">About Train, validation and test sets</a></a></span></li><li><span><a href=\"#Today-dataset\" data-toc-modified-id=\"Today-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Today dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-dataset\" data-toc-modified-id=\"Load-the-dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load the dataset</a></span></li><li><span><a href=\"#Build-X-(features-vectors)-and-y-(labels)\" data-toc-modified-id=\"Build-X-(features-vectors)-and-y-(labels)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Build X (features vectors) and y (labels)</a></span></li></ul></li><li><span><a href=\"#Build-a-Baseline\" data-toc-modified-id=\"Build-a-Baseline-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build a Baseline</a></span></li><li><span><a href=\"#A-better-classifier-with-a-preprocessing\" data-toc-modified-id=\"A-better-classifier-with-a-preprocessing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>A better classifier with a preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-pre-processing\" data-toc-modified-id=\"Some-pre-processing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Some pre-processing</a></span></li><li><span><a href=\"#2.4.-Search-hyper-parameters\" data-toc-modified-id=\"2.4.-Search-hyper-parameters-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>2.4. Search hyper-parameters</a></span></li></ul></li><li><span><a href=\"#3.-Summarize-your-conclusion-here\" data-toc-modified-id=\"3.-Summarize-your-conclusion-here-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>3. Summarize your conclusion here</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCNRejgk2CqQ"
   },
   "source": [
    "# Case Study: Sentiment Analysis\n",
    "\n",
    "Text classification is a machine learning technique that assigns a set of predefined categories to open-ended text. Text classifiers can be used to organize, structure, and categorize pretty much any kind of text – from documents, medical studies and files, and all over the web.\n",
    "\n",
    "For example, new articles can be organized by topics; support tickets can be organized by urgency; chat conversations can be organized by language; brand mentions can be organized by sentiment; and so on.\n",
    "\n",
    "Text classification is one of the fundamental tasks in natural language processing with broad applications such as **sentiment analysis**, topic labeling, spam detection, and intent detection.\n",
    "\n",
    "**Why is Text Classification Important?**\n",
    "\n",
    "It’s estimated that around 80% of all information is unstructured, with text being one of the most common types of unstructured data. Because of the messy nature of text, analyzing, understanding, organizing, and sorting through text data is hard and time-consuming, so most companies fail to use it to its full potential.\n",
    "\n",
    "This is where text classification with machine learning comes in. Using text classifiers, companies can automatically structure all manner of relevant text, from emails, legal documents, social media, chatbots, surveys, and more in a fast and cost-effective way. This allows companies to save time analyzing text data, automate business processes, and make data-driven business decisions.\n",
    "\n",
    "**How Does Text Classification Work?**\n",
    "\n",
    "Instead of relying on manually crafted rules, machine learning text classification learns to make classifications based on past observations. By using pre-labeled examples as training data, machine learning algorithms can learn the different associations between pieces of text, and that a particular output (i.e., tags) is expected for a particular input (i.e., text). A “tag” is the pre-determined classification or category that any given text could fall into.\n",
    "\n",
    "The first step towards training a machine learning NLP classifier is feature extraction: a method is used to transform each text into a numerical representation in the form of a vector. One of the most frequently used approaches is bag of words, where a vector represents the frequency of a word in a predefined dictionary of words.\n",
    "\n",
    "Then, the machine learning algorithm is fed with training data that consists of pairs of feature sets (vectors for each text example) and tags (e.g. sports, politics) to produce a classification model:\n",
    "\n",
    "![training](https://monkeylearn.com/static/507a7b5d0557f416857a038f553865d1/2ed04/text_process_training.webp)\n",
    "\n",
    "Once it’s trained with enough training samples, the machine learning model can begin to make accurate predictions. The same feature extractor is used to transform unseen text to feature sets, which can be fed into the classification model to get predictions on tags (e.g., sports, politics):\n",
    "\n",
    "![prediction](https://monkeylearn.com/static/afa7e0536886ee7152dfa4c628fe59f0/2b924/text_process_prediction.webp)\n",
    "\n",
    "Text classification with machine learning is usually much more accurate than human-crafted rule systems, especially on complex NLP classification tasks. Also, classifiers with machine learning are easier to maintain and you can always tag new examples to learn new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:20:40.675514Z",
     "start_time": "2022-01-17T09:20:40.673229Z"
    }
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:20:41.138987Z",
     "start_time": "2022-01-17T09:20:41.135091Z"
    },
    "id": "CqS_yFTz2Cqd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pylab as pl # package inheriting most of matplotlib package functions with shorter syntax \n",
    "import seaborn as sns \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:20:41.993271Z",
     "start_time": "2022-01-17T09:20:41.989875Z"
    },
    "id": "H2LHlNq02CqX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(Practical tip) Table of contents can be compiled directly in jupyter notebooks using the following code:\n",
    "I set an exception: if the package is in your installation you can import it otherwise you download it \n",
    "then import it.\n",
    "\"\"\"\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu \n",
    "except:\n",
    "    !pip install jyquickhelper\n",
    "    from jyquickhelper import add_notebook_menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeksHxZ52Cqe"
   },
   "source": [
    "## [About Train, validation and test sets](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
    "![test/train/val](https://miro.medium.com/max/1466/1*aNPC1ifHN2WydKHyEZYENg.png)\n",
    "\n",
    "* **Training Dataset:** The sample of data used to fit the model.\n",
    "* **Validation Dataset:** The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "* **Test Dataset:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:29:51.928813Z",
     "start_time": "2022-01-17T09:29:51.924454Z"
    },
    "id": "cJY3IDY42Cqa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Output Table of contents to navigate easily in the notebook. \n",
    "For interested readers, the package also includes Ipython magic commands to go back to this cell\n",
    "wherever you are in the notebook to look for cells faster\n",
    "\"\"\"\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQvQni-N2Cqc"
   },
   "source": [
    "## Today dataset\n",
    "\n",
    "In this lab we use part of the 'Amazon_Unlocked_Mobile.csv' dataset published by Kaggle. The dataset contain the following information:\n",
    "* Product Name\n",
    "* Brand Name\n",
    "* Price\n",
    "* Rating\n",
    "* Reviews\n",
    "* Review Votes\n",
    "\n",
    "We are mainly interested by the 'Reviews' (X) and by the 'Rating' (y)\n",
    "\n",
    "The goal is to try to predict the 'Rating' after reading the 'Reviews'. I've prepared for you TRAIN and TEST set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2Ti1vXd2Cqc"
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:30:12.335772Z",
     "start_time": "2022-01-17T09:30:12.001594Z"
    },
    "id": "zqotUQob2Cqe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy Note 4 N910C Unlocked Cellphone...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>449.99</td>\n",
       "      <td>4</td>\n",
       "      <td>I love it!!! I absolutely love it!! 👌👍</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLU Energy X Plus Smartphone - With 4000 mAh S...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>139.00</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the BLU phones! This is my second one t...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple iPhone 6 128GB Silver AT&amp;T</td>\n",
       "      <td>Apple</td>\n",
       "      <td>599.95</td>\n",
       "      <td>5</td>\n",
       "      <td>Great phone</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLU Advance 4.0L Unlocked Smartphone -US GSM -...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>51.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Very happy with the performance. The apps work...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huawei P8 Lite US Version- 5 Unlocked Android ...</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>198.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Easy to use great price</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BLU WIN HD LTE - 5.0\" Windows Smartphone -GSM ...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>109.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Came faster then expected, thanks cellathon. I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BLU Dash Jr 3G Unlocked Phone - Retail Packagi...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>39.97</td>\n",
       "      <td>1</td>\n",
       "      <td>Phone stopped working within 3 days!!</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Samsung Galaxy S6 Edge Plus SM-G928 32GB Black...</td>\n",
       "      <td>samsung</td>\n",
       "      <td>557.49</td>\n",
       "      <td>1</td>\n",
       "      <td>ordered for a new phone and got a used one wit...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZTE Axon Pro - Factory Unlocked Phone, 32 GB I...</td>\n",
       "      <td>ZTE</td>\n",
       "      <td>419.99</td>\n",
       "      <td>5</td>\n",
       "      <td>A+</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LG G4 Unlocked Smartphone with 32GB Internal M...</td>\n",
       "      <td>LG</td>\n",
       "      <td>324.84</td>\n",
       "      <td>1</td>\n",
       "      <td>Lots of problems with this phone. There's no \"...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  Samsung Galaxy Note 4 N910C Unlocked Cellphone...    Samsung  449.99   \n",
       "1  BLU Energy X Plus Smartphone - With 4000 mAh S...        BLU  139.00   \n",
       "2                   Apple iPhone 6 128GB Silver AT&T      Apple  599.95   \n",
       "3  BLU Advance 4.0L Unlocked Smartphone -US GSM -...        BLU   51.99   \n",
       "4  Huawei P8 Lite US Version- 5 Unlocked Android ...     Huawei  198.99   \n",
       "5  BLU WIN HD LTE - 5.0\" Windows Smartphone -GSM ...        BLU  109.99   \n",
       "6  BLU Dash Jr 3G Unlocked Phone - Retail Packagi...        BLU   39.97   \n",
       "7  Samsung Galaxy S6 Edge Plus SM-G928 32GB Black...    samsung  557.49   \n",
       "8  ZTE Axon Pro - Factory Unlocked Phone, 32 GB I...        ZTE  419.99   \n",
       "9  LG G4 Unlocked Smartphone with 32GB Internal M...         LG  324.84   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       4             I love it!!! I absolutely love it!! 👌👍           0.0  \n",
       "1       5  I love the BLU phones! This is my second one t...           4.0  \n",
       "2       5                                        Great phone           1.0  \n",
       "3       4  Very happy with the performance. The apps work...           2.0  \n",
       "4       5                            Easy to use great price           0.0  \n",
       "5       5  Came faster then expected, thanks cellathon. I...           NaN  \n",
       "6       1              Phone stopped working within 3 days!!           3.0  \n",
       "7       1  ordered for a new phone and got a used one wit...           0.0  \n",
       "8       5                                                 A+           0.0  \n",
       "9       1  Lots of problems with this phone. There's no \"...           4.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/train.csv.gz\")\n",
    "VAL = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/val.csv.gz\")\n",
    "TEST = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/test.csv.gz\")\n",
    "\n",
    "TRAIN.head(10)#pass an integer value to specify the number of header lines to output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMazEWld2Cqq"
   },
   "source": [
    "### Build X (features vectors) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:30:49.714573Z",
     "start_time": "2022-01-17T09:30:49.707060Z"
    },
    "id": "LK6otSJU2Cqq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000,), (5000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_train and y_train\n",
    "X_train = TRAIN['Reviews'].fillna(\"\")\n",
    "y_train = TRAIN['Rating']\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:30:50.164033Z",
     "start_time": "2022-01-17T09:30:50.157926Z"
    },
    "id": "1u-bbHc32Cqr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_val and y_val\n",
    "X_val = VAL['Reviews'].fillna(\"\")\n",
    "y_val = VAL['Rating']\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:30:50.655660Z",
     "start_time": "2022-01-17T09:30:50.650288Z"
    },
    "id": "WMZtc-wc2Cqr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_test and y_test\n",
    "X_test = TEST['Reviews'].fillna(\"\")\n",
    "y_test = TEST['Rating']\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX1lUZ1Q2Cqr"
   },
   "source": [
    "## Build a Baseline\n",
    "Using a binary `CountVectorizer` and a `LogisticRegression` classifier, learned in a previous lecture, build a first model.\n",
    "\n",
    "For this model, you will not pre-process the text and will only use words (not N-grams). Leaves all parameter as default.\n",
    "\n",
    "The evaluation metric is accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBO3PedQ2Cqs"
   },
   "source": [
    "$$[TO DO - Students]$$\n",
    "\n",
    "Explain in your own words both classification models (CountVectorizer & Logistic Regression), feel free to specify key features/hyperparameters of these models that seem the most important ones to tune and explain why. You can include it at the end of the notebook for your conclusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:31:04.634122Z",
     "start_time": "2022-01-17T09:31:04.251450Z"
    },
    "id": "SGiYlqFJ2Cqs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 8991)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode X_train\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X_train_encoded = cv.transform(X_train)\n",
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JjFPaQlJ2Cqs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taylorlucero/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of validation 5066\n",
      "vocabulary size of train 13099\n",
      "Size of flatten reviews of Validation 42300\n",
      "Size of flatten reviews of Train 228642\n"
     ]
    }
   ],
   "source": [
    "# What is the vocabulary size ?\n",
    "# Compare with your previous response\n",
    "\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt') # If nltk requires to download 'punkt' depending on your installation\n",
    "reviews_tokenized = [word_tokenize(review) for review in X_train]\n",
    "\n",
    "flatten_reviews = [item for sublist in reviews_tokenized for item in sublist] # contain all words \n",
    "unique_words = list(set(flatten_reviews)) #processed as a list for future analysis\n",
    "vocabulary_size = len(unique_words) # set allows to get unique words contain in flatten_reviews\n",
    "\n",
    "run_failure = False #True\n",
    "if run_failure: #This code will give you an error\n",
    "    reviews_tokenized_val = [word_tokenize(review) for review in X_val]\n",
    "    for sentence in reviews_tokenized_val[:10]: print('%s \\n'%sentence)\n",
    "else: # Here for practical reasons we just show how to deal with this issue using exceptions\n",
    "    reviews_tokenized_val= []\n",
    "    failed_review=[]\n",
    "    for review in X_val:\n",
    "        try: # Store the result of the function word_tokenize applied on current review if it does not fail \n",
    "            reviews_tokenized_val.append(word_tokenize(review))\n",
    "        except:\n",
    "            failed_review.append(review)\n",
    "            continue\n",
    "\n",
    "flatten_reviews_val = [item for sublist in reviews_tokenized_val for item in sublist] # contain all words \n",
    "unique_words_val = list(set(flatten_reviews_val)) #processed as a list for future analysis\n",
    "vocabulary_size_val = len(unique_words_val) # set allows to get unique words contain in flatten_reviews\n",
    "print('Vocabulary size of validation', vocabulary_size_val)\n",
    "print('vocabulary size of train', vocabulary_size)\n",
    "print('Size of flatten reviews of Validation', len(flatten_reviews_val))\n",
    "print('Size of flatten reviews of Train', len(flatten_reviews))\n",
    "# [TO DO - Students] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ubIzPYN52Cqt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/taylorlucero/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the stopwords: \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "179\n",
      "These are the stopwords used in the train:\n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'm', 'o', 're', 'y', 'didn', 'doesn', 'haven', 'ma', 'wasn', 'wouldn'] \n",
      "136\n",
      "These are the stopwords used in the val: \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'm', 'o', 're', 'y', 'didn', 'doesn', 'haven', 'ma', 'wasn', 'wouldn']  \n",
      "122\n"
     ]
    }
   ],
   "source": [
    "# What is the stop words used\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "print('These are the stopwords: \\n %s ' %(stopWords))\n",
    "print(len(stopWords))\n",
    "\n",
    "usedWords = [a for a in stopWords if a in flatten_reviews ]\n",
    "print('These are the stopwords used in the train:\\n %s ' %(usedWords))\n",
    "print(len(usedWords))\n",
    "\n",
    "usedWords_val = [ i  for i in stopWords if i in flatten_reviews_val ]\n",
    "print('These are the stopwords used in the val: \\n %s  ' %(usedWords))\n",
    "print(len(usedWords_val))\n",
    "# [TO DO - Students] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7KzN0-sr2Cqt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "(1, 6)\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "(1, 6)\n",
      "Vocabulary:  {'love': 3, 'the': 5, 'chocolate': 1, 'but': 0, 'hate': 2, 'tea': 4}\n",
      "[[1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Transform and then inverse transform the following text\n",
    "# Interpret the result\n",
    "text = \"I love the chocolate but I hate tea\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit([text])\n",
    "vectorized_text = vectorizer.transform([text])\n",
    "print(vectorized_text)\n",
    "print(vectorized_text.shape)\n",
    "\n",
    "decoded = [vectorizer.decode(vectorized_text) for i in vectorized_text]\n",
    "print(vectorizer.decode(vectorized_text))\n",
    "print(vectorizer.decode(vectorized_text).shape)\n",
    "\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "print(vectorized_text.toarray())\n",
    "# [TO DO - Students] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:31:53.231409Z",
     "start_time": "2022-01-17T09:31:53.229258Z"
    },
    "id": "dP2JB3QR2Cqt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using LogisticRegression from sklearn, fit a first model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Keep default settings we will look into hyperparameters fine-tuning later on\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_encoded,y_train)\n",
    "# [TO DO - Students] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:31:53.430102Z",
     "start_time": "2022-01-17T09:31:53.428392Z"
    },
    "id": "mPIoyA5H2Cqu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 8991)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode X_test\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#cv = CountVectorizer()\n",
    "#cv.fit(X_test)\n",
    "X_test_encoded = cv.transform(X_test)\n",
    "X_test_encoded.shape\n",
    "# [TO DO - Students] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MIksonBA2Cqu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.64      0.64       159\n",
      "           2       0.29      0.16      0.20        51\n",
      "           3       0.21      0.15      0.18        71\n",
      "           4       0.43      0.26      0.33       163\n",
      "           5       0.76      0.90      0.82       556\n",
      "\n",
      "    accuracy                           0.67      1000\n",
      "   macro avg       0.47      0.42      0.44      1000\n",
      "weighted avg       0.62      0.67      0.64      1000\n",
      "\n",
      "Accuracy: 0.666\n"
     ]
    }
   ],
   "source": [
    "# Using classification_report, evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "print(\"classification_report\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# [TO DO - Students] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1K-1HTE2Cqu"
   },
   "source": [
    "$$[ TO DO - Students]$$\n",
    "\n",
    "What do these outputted classsification metrics mean? Interpret these results.  You can include it at the end of the notebook for your conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of examples in each class are different and our classes are uneven, f1-score is the best metric to evaluate our model. And it shows that when we have a higher number of examples in the class, we can get a higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVuZV9x62Cqv"
   },
   "source": [
    "## A better classifier with a preprocessing\n",
    "\n",
    "It's up to you. Try to get a better score (accuracy) using what we have seen in this course:\n",
    "- efficient text pre-processing\n",
    "- choice of feature extraction\n",
    "- use of a more powerful classifier or better hyper-parameter for LogisticRegression.\n",
    "\n",
    "The training of the model must be done on the Train and the evaluation on the Test. You can of course use GridSearchCV or RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9rPR4BZ2Cqv"
   },
   "source": [
    "###  Some pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:31:56.205903Z",
     "start_time": "2022-01-17T09:31:56.202127Z"
    },
    "id": "Zp-1Tl7c2Cqv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/taylorlucero/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Write a \"clean_text\" function that accepts a text as input and returns a clean text.\n",
    "# The possible steps (take what you want) are:\n",
    "# - removal of emoji\n",
    "# - lowercase the text\n",
    "# - remove punctuation\n",
    "# - remove words containing numbers\n",
    "# - remove stop words\n",
    "# - stemming or lemmatization\n",
    "# - remove words smaller than nfrom nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "try:\n",
    "    import emoji\n",
    "except:\n",
    "    !pip install emoji\n",
    "    import emoji\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer().stem\n",
    "lemmatizer = WordNetLemmatizer().lemmatize\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # [TO DO - Students] Create your own preprocessing function\n",
    "    import re\n",
    "    \n",
    "\n",
    "    \n",
    "    def remove_emoji(text):\n",
    "        \n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\" \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\" \n",
    "                           u\"\\u3030\"    \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'',text)     #removing emoji\"\"\"\n",
    "        text = remove_emoji(text)\n",
    "        text = text.lower()  #lowercasing the text\n",
    "        text = re.sub(r'[^\\w\\s]','',text)        #removing punctuation\n",
    "           \n",
    "        tk = WhitespaceTokenizer()\n",
    "        text_tokens = tk.tokenize(text)       \n",
    "                        #removing stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "        text_without_stwords = [w for w in text_tokens if not w in stop_words] \n",
    "        text = \" \".join(text_without_stwords)       \n",
    "            \n",
    "        text = re.sub(r'[0-9]+', '', text)   #removing words containing numbers\n",
    "     \n",
    "        text = re.sub(r'\\b\\w{1,2}\\b', '', text)     #removing words less than 3 chraracter \n",
    "      \n",
    "  \n",
    "\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "       #lemmatizing the text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:31:56.879259Z",
     "start_time": "2022-01-17T09:31:56.874183Z"
    },
    "id": "5hfG7bdO2Cqw"
   },
   "outputs": [],
   "source": [
    "# Clean, X_train and X_test with the previous preprocessing\n",
    "X_train_cleaned = [clean_text(r) for r in X_train]\n",
    "X_test_cleaned = [clean_text(r) for r in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T09:31:58.871696Z",
     "start_time": "2022-01-17T09:31:57.816104Z"
    },
    "id": "RdlYMM5w2Cqw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Build, Fit and Evaluate a model using the previous preprocessing\n",
    "# Did you improve the result ? \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cv = CountVectorizer(analyzer='word', binary=True)\n",
    "cv.fit(X_train_cleaned)\n",
    "X_train_encoded = cv.transform(X_train_cleaned)\n",
    "\n",
    "X_test_encoded = cv.transform(X_test_cleaned)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "y_pred = lr.predict(X_test_encoded)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jUuzJuA2Cqx"
   },
   "source": [
    "### 2.4. Search hyper-parameters\n",
    "\n",
    "![GridSearch](https://i.stack.imgur.com/81Yoo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "NG803sda2Cqx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=0; total time=   2.5s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=0; total time=   2.4s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=0; total time=   2.4s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=1; total time=   2.4s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=1; total time=   2.6s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=1; total time=   2.5s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=2; total time=   2.4s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=2; total time=   2.1s\n",
      "[CV] END classification__class_weight=None, classification__solver=lbfgs, feature_extraction__min_df=2; total time=   2.4s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=0; total time=   1.8s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=0; total time=   1.9s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=0; total time=   2.0s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=1; total time=   2.0s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=1; total time=   2.0s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=1; total time=   1.9s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=2; total time=   1.9s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=2; total time=   1.8s\n",
      "[CV] END classification__class_weight=None, classification__solver=liblinear, feature_extraction__min_df=2; total time=   1.8s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=0; total time=   2.4s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=0; total time=   2.4s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=0; total time=   2.5s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=1; total time=   2.5s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=1; total time=   2.5s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=1; total time=   2.5s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=2; total time=   2.2s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=2; total time=   2.3s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=lbfgs, feature_extraction__min_df=2; total time=   2.4s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=0; total time=   2.0s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=0; total time=   2.0s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=0; total time=   2.1s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=1; total time=   2.0s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=1; total time=   1.9s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=1; total time=   1.9s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=2; total time=   2.1s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=2; total time=   2.0s\n",
      "[CV] END classification__class_weight=balanced, classification__solver=liblinear, feature_extraction__min_df=2; total time=   1.9s\n"
     ]
    }
   ],
   "source": [
    "# Using Grid Search or Random Search try to find some better hyperparameters\n",
    "from nltk import word_tokenize          \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "# I use my own tokenizer\n",
    "class MyTokenizer:\n",
    "    def __call__(self, doc):\n",
    "        return [clean_text(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# I define the pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('feature_extraction',  TfidfVectorizer(tokenizer=MyTokenizer())),\n",
    "        ('classification',  LogisticRegression(multi_class='auto', max_iter=400))\n",
    "        ])\n",
    "\n",
    "# I define the parameter space\n",
    "# [TO DO - Students] Set the parameters to browse of your choice for the GridSearchCV\n",
    "parameters = {}\n",
    "#parameters['feature_extraction__ngram_range'] = [ (1,2),(1,1), (1,3)]\n",
    "parameters['feature_extraction__min_df'] = [0,1,2] #[0,1,5,8]\n",
    "parameters['feature_extraction__max_features'] = [1,5,8]\n",
    "\n",
    "parameters['classification__class_weight'] = [None, 'balanced']\n",
    "parameters['classification__solver'] = ['lbfgs', 'liblinear']\n",
    "\n",
    "\n",
    "# I use GridSearchCV to search best hyper parameter\n",
    "# I use RandomizedSearchCV to search good hyper parameter\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=3, verbose=2,n_jobs=None)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGJC0fZz2Cqx"
   },
   "source": [
    "## Summarize your conclusion here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "9H3XSpN_2Cqy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification__class_weight': None,\n",
       " 'classification__solver': 'lbfgs',\n",
       " 'feature_extraction__min_df': 0}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is your best params\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "cWwl8eok2Cqy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6607983325303727"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is your best score\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "SqqNBzQ62Cqy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_extraction',\n",
       "                 TfidfVectorizer(min_df=0,\n",
       "                                 tokenizer=<__main__.MyTokenizer object at 0x7fc12449ac40>)),\n",
       "                ('classification', LogisticRegression(max_iter=400))])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is your best estimator\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePj7XyI12Cqz"
   },
   "source": [
    "A random draw with balanced classes (as much data from each class) would give an accuracy of 20% (1/5). We notice that a very light pre-processing or even no pre-processing already gives much better results (around 65-66%) and that it is not easy to do much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "BCmtw3AU2Cqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print/plot the confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(grid, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0qxciA62Cqz"
   },
   "source": [
    "The confusion matrix helps us understand the quality of the results. On the diagonal we find the quality of the predictions for a given class:\n",
    "* Class 1 is found around 70 %.\n",
    "* Class 2, 3 have a low score < 10%.\n",
    "* Class 4 is found around 16-20 %\n",
    "* Class 5 is found more than 90 %\n",
    "\n",
    "On one line is the way the original class was found. For example for class 2 (depend the run) :\n",
    "* at 59%, the predictor says that a data of this class is of class 1\n",
    "* at 4% of the right class (the 2)\n",
    "* at 2% of class 3\n",
    "* at 4% of class 4\n",
    "* at 31% of class 5\n",
    "The sum is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own words: \n",
    "\n",
    "The CountVectorizer classification model,uses the CountVectorizer function to transfrom words, sentences, \n",
    "or documetns into a vector based on the frequency of each word in the document. The CountVectorizer gives a general fitted model based on the frequency a term appears in the document. It creates a matrix consting of the unique words that are represented by each column of the matrix and the frequency by values in the rows. \n",
    "This is made possible through Speech marking (POS) and Named Entity Recognition. The produced matrix is a\n",
    "sparse matrix since it contains alot of zeros due to the frequency of words that dont appear in a that \n",
    "specific referenced instance. Parameters for this model include, removal of stop words, lowercase conversion, \n",
    "lemmatization or stemming, and the range of the ngram. Stop words are removed which can change the meaning or \n",
    "certain expressions and also changes the dimensionality of the data. Lowercase conversion does not change the \n",
    "dimensionaltiy of the data but converts all characters into lowercase form in an attmept to normalize data. \n",
    "Lemmatization and stemming, the former converts varainces and inflections into their base form and always produces\n",
    "a known word, the latter can sometimes create new words or unkwown words due to its process of only removing suffixes\n",
    "and sometimes prefixes. The ngram is important becuase it groups words together. In this case since it si based on frequency words that are appearing less frequently would be harder for the model to fit. While words that appear more frequently would show to have larger accuracy. \n",
    "\n",
    "It ran with ngram_range and max features, and was providing an accuracy of .53 however, in the confusion everything was zero. By removing some of new tuned parameters, the accuracy slowly increased to .66, while the confusion matrix began to show values greater than zero in the Confusion matrix cells. In this case values of 2 ,3 & 4 were the worst cases while 1 and 5 were the highest, with 5 being the greatest at 529\n",
    "\n",
    "The Logistic Regression classification model,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VQvQni-N2Cqc",
    "NVuZV9x62Cqv"
   ],
   "name": "01-notebook-NLP-sentiment-analysis-with-LR-students.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
