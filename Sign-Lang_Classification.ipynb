{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6dAHREpe_DY"
      },
      "source": [
        "# Sign Langauge Alphabet Classifier using CV2\n",
        "## Taylor Lucero \n",
        "# Computer Vision and Machine Learning \n",
        "\n",
        "https://drive.google.com/drive/folders/1UDRvzc6WuSL_vsn8MsmEXp_IQMkmqdMY?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DztrxiOEj0IG"
      },
      "outputs": [],
      "source": [
        "#Helpful imported packages\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from math import cos, sin, sqrt\n",
        "import io\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import figure\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "#from cv2 import videoCaptureObject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26sF0g_gCsNW"
      },
      "source": [
        "JavaScript code to create a real time video stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "IRYtIBo2kKUk",
        "outputId": "379b56ca-e389-4a6d-cf60-006fde8002ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', .90);\n",
              "\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a real time video stream\n",
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', .90);\n",
        "\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "maxwidth = 640\n",
        "maxheight = 480 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jhxjH_fCGnX"
      },
      "source": [
        "Helpful User Defined Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W69pECUzkQfc"
      },
      "outputs": [],
      "source": [
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "# this function converts bytes to images. Bytes are received from the captured video stream. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmbTyFlAkRjF"
      },
      "outputs": [],
      "source": [
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x\n",
        "\n",
        "  # This function converts images to Bytes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zKnxMCTkWh-"
      },
      "outputs": [],
      "source": [
        "def detect(img, cascade):\n",
        "    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    if len(rects) == 0:\n",
        "        return []\n",
        "    rects[:,2:] += rects[:,:2]\n",
        "    return rects\n",
        "  \n",
        "# This function detects an images using the haarcascade. If there is no detected face it return an empty list, otherwise it returns a rectangle (points)\n",
        "# that surround your face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odpv1dbjkZVN"
      },
      "outputs": [],
      "source": [
        "def draw_rects(im, rects, color):\n",
        "    for x1, y1, x2, y2 in rects:\n",
        "        cv2.rectangle(im, (x1, y1), (x2, y2), color, 3)\n",
        "# this function draws the Rectangle. This will be used to draw the rectangle around your face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8Mn4ZPKubd5"
      },
      "outputs": [],
      "source": [
        "def compute_coordinates(rect): #[x1,y1,x2,y2]\n",
        "\n",
        "  x1=max(0,rect[0]-margin)\n",
        "  y1=max(0,rect[1]-margin)\n",
        "  x2=min(rect[2]+margin,maxwidth)\n",
        "  y2=min(rect[3]+margin,maxheight)\n",
        "  \n",
        "  return [x1,y1,x2,y2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir-desIW50Fo"
      },
      "outputs": [],
      "source": [
        "#implement\n",
        "def rest(rect): #[x1,y1,x2,y2]\n",
        "\n",
        "  x1=min(maxwidth,rect[0]+restr)\n",
        "  y1=min(maxheight,rect[1]+restr)\n",
        "  x2=max(rect[2]-restr,x1)\n",
        "  y2=max(rect[3]-restr,y1)\n",
        "  return [x1,y1,x2,y2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh35RSvQCQvG"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "Develop a program that identifies your face in the the captured video stream using the Haarcascade classifier. Optionally, detect the eyes on your face. Once the face is detected draw a rectangle around your face and your eyes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWGO3RQ1CFRj"
      },
      "source": [
        "# Task 2 \n",
        "\n",
        "Improve the computation speed of the face detection, by first identifying the face and then defining a new larger region of interest around your identified face. This new region of interest surround the face will then have the detection fucntion applied to only work with that ROI. This reduced the area where the detection function is applied making it more efficient in identifying the face and eyes. This application makes sense a s head does not tend drastically move across a screen quickly, it will slowly travers across it. With this in mind. When the face movs in the larger detected region the region will then apply the detect function applied over and recompute where the larger ROI should be moved to in alignment with the new detected face position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a1gldgDsGg9"
      },
      "source": [
        "# Task 3 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExek2KksEQm"
      },
      "source": [
        "Identify the face in the captured video stream, then calculate the histogram from the gray image. Then use back propagation on the constructed histogram.  Essentially, it creates an image of the same size (but single channel) as that of our input image, where each pixel corresponds to the probability of that pixel belonging to our object. In more simpler words, the output image will have our face of interest in more white compared to other parts of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yezBVSGOFSBQ"
      },
      "source": [
        "# Task 4 \n",
        "After calculating the probability of the face using backprojection and the histogram, set the probability of the identified face to zero. Since the program will iterate through till a face is found, as your identified face will no long be recognized by the program, it will search for other parts of the image that share similar statistics based on the hues and colors of the pixels. Thus only other identifiable object in the video stream that shares similarities to your face would be your hand, forcing the program to recognize it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KAfX4UQVdJO"
      },
      "source": [
        "# Task 5 and Task 6\n",
        "\n",
        "Detect and Store your Hand \n",
        "\n",
        "After removing my face (setting the probability to 0) Cam shift will detect your hand since the skin color probability distribution of your hand is very similar to your face. \n",
        "\n",
        "Image should be stored in 2 sizes:\n",
        "\n",
        "\n",
        "*   16 x 16\n",
        "*   224 x 224 \n",
        "\n",
        "Link to drive:\n",
        "\n",
        "https://drive.google.com/drive/folders/1UDRvzc6WuSL_vsn8MsmEXp_IQMkmqdMY?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3cP6ucuPvGb",
        "outputId": "10e3a2ac-75c9-4e47-8b83-91418d6cb54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQSK214tseY6"
      },
      "outputs": [],
      "source": [
        "# initialize storage \n",
        "storage=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rzq-6dWqTViZ",
        "outputId": "1a28cf92-6b00-460c-c056-006e790270c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', .90);\n",
              "\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many pictures do you want? 1\n",
            "How many seconds between each photo? 1\n",
            "Starting 1 second timer for Picture 1\n",
            "Would you like to keep this picture?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x139 at 0x7FA9C1B9CF50>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keep Y or N: y\n",
            "Picture 1 was stored\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#A T C\n",
        "\n",
        "VideoCapture() \n",
        "eval_js('create()') \n",
        "\n",
        "Cascade_visage = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "FDetect=False\n",
        "margin = 30\n",
        "restr = 30\n",
        "\n",
        "Picture_count=int(input(\"How many pictures do you want? \"))# the number of picture to be taken\n",
        "Time=int(input(\"How many seconds between each photo? \"))\n",
        "x=0\n",
        "picture_store =[]\n",
        "\n",
        "\n",
        "while not FDetect:\n",
        "  bigrect=[] \n",
        "  byte = eval_js('capture()')\n",
        "  #norm_rgb = normalized(byte)\n",
        "  im = byte2image(byte)\n",
        "  #norm_rgb = normalized(im)\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
        "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
        "  face = detect(gray, cascade = Cascade_visage) # face contains the rectangle\n",
        "  \n",
        "  if len(face)==0:\n",
        "    FDetect=False\n",
        "  else:\n",
        "    FDetect=True\n",
        "\n",
        "    \n",
        "  if FDetect: # if a face is found\n",
        "\n",
        "  \n",
        "    r_face = rest(face[0])\n",
        "    hsv_roi=hsv[r_face[1]:r_face[3], r_face[0]:r_face[2]] # !!! better to restrict this face for the histogram\n",
        "    mask_roi = cv2.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "    kernel= cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,1))\n",
        "    mask_roi = cv2.morphologyEx(mask_roi, cv2.MORPH_CLOSE, kernel) #removes unnecessary black noises from the white region.\n",
        "    #Closing is reverse of Opening, Dilation followed by Erosion.\n",
        "    mask_roi = cv2.morphologyEx(mask_roi, cv2.MORPH_OPEN, kernel) #removes white noise from the black region of the mask.\n",
        "    #Opening is just another name of erosion followed by dilation.\n",
        "    bigrect.append(compute_coordinates(face[0])) # compute bigrect and add to the list\n",
        "\n",
        "    Detection=True # set a flag to start camshift\n",
        "\n",
        "\n",
        "    hist=cv2.calcHist([hsv_roi], [0], mask_roi, [180], [0,180]) # compute the histogram\n",
        "\n",
        "    # Region of interest identified\n",
        "    \n",
        "    hist=cv2.normalize(hist, hist,  alpha=0, beta=255, norm_type=cv2.NORM_MINMAX) # normalize the histogram \n",
        "\n",
        "  \n",
        "    track_window = (face[0][0], face[0][1], face[0][2], face[0][3])\n",
        "   # sets some criteria to iterate 10 times\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 ) ##\n",
        "\n",
        "\n",
        "while Detection:\n",
        "\n",
        "\n",
        "    byte = eval_js('capture()')\n",
        "    im = byte2image(byte)\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
        "    hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #\n",
        "    mask = cv2.inRange(hsv, np.array((0,64,32)), np.array((180,200,200)))\n",
        "    # (0,64,32) ( 180, 200,200)\n",
        "      # compute the region of interest (bigrect)\n",
        "    raw_im = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
        "    prob = cv2.calcBackProject([hsv], [0], hist, [0, 180], 1) \n",
        "\n",
        "      \n",
        "\n",
        "      # perform camshift over the track_window\n",
        "\n",
        "      \n",
        "    \n",
        "    for i in range(bigrect[0][0], bigrect[0][2]): # iteration  between coordinates\n",
        "      for j in range(bigrect[0][1], bigrect[0][3]): \n",
        "        prob[j,i]=0 # sets probability matrix to 0 on all points inside bigrect\n",
        "\n",
        "    prob &= mask\n",
        "\n",
        "    track_box, track_window = cv2.CamShift(prob, track_window, term_crit)\n",
        "\n",
        "    im[:] = prob[...,np.newaxis] # add probability matrix to the image\n",
        "\n",
        "\n",
        "    cv2.rectangle(im, track_window , (255,255,255), 1)\n",
        "\n",
        "\n",
        "    eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
        "    \n",
        "    if x == Picture_count:\n",
        "      break\n",
        "    else:\n",
        "      print(\"Starting\", Time, \"second timer for Picture\", x+1)\n",
        "      time.sleep(Time)\n",
        "      \n",
        "    print(\"Would you like to keep this picture?\")\n",
        "   \n",
        "  \n",
        "# Crop image\n",
        "    cropped_image = im[int(track_window[1]):int(track_window[1]+track_window[3]), \n",
        "                          int(track_window[0]):int(track_window[0]+track_window[2]+)]\n",
        "\n",
        "    cv2_imshow(cropped_image)\n",
        "    keep_remove=input(\"Keep Y or N: \")\n",
        "    \n",
        "# Store the image\n",
        "    if keep_remove == 'y':\n",
        "\n",
        "      picture_store.append(cropped_image)\n",
        "      print(\"Picture\",x+1,\"was stored\")\n",
        "      x+=1\n",
        "      continue\n",
        "\n",
        "\n",
        "    elif keep_remove =='n':\n",
        "      x+=1\n",
        "      continue\n",
        "\n",
        "    else:\n",
        "      print(\"Picture Storage is being cleaned\")\n",
        "      picture_store =[]\n",
        "      cropped_image = 0 \n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "j6Qj_GOX_BEl",
        "outputId": "312d70b1-3132-4f61-ff34-0942442e18c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x139 at 0x7FA9C1CBF850>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Test picture store kept the picture\n",
        "\n",
        "print(len(picture_store))\n",
        "storage.extend(picture_store)\n",
        "print(len(storage))\n",
        "cv2_imshow(picture_store[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHg4KquS4loJ"
      },
      "source": [
        "# Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKLJva5iegF"
      },
      "source": [
        "Add the Text file line shuffler "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1HCn7VIlAOw"
      },
      "outputs": [],
      "source": [
        "# This function saves the image in 16x16 and 224x224 in the set format\n",
        "# returns Letter, ID, Size\n",
        "#letter_id_size.jpg\n",
        "def Picture_save(storage): \n",
        "  # Sets the paths \n",
        "  path = \"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/\" \n",
        "  Letter = input(\"Letter\")\n",
        "\n",
        "  Size= [16, 224] # To create an image in both 16 x16 and 224 x224\n",
        "  for ID,image in enumerate(storage):\n",
        "    for b in Size:\n",
        "      file_name= Letter.upper()+\"_\" + str(ID) +\"_\"+  str(b)\n",
        "      ### Changing the size of the picture\n",
        "      im_size = cv2.resize(image, (b,b) , interpolation = cv2.INTER_AREA)\n",
        "      cv2.imwrite(path + file_name + \".jpg\", im_size)\n",
        "      print(path+file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wPZjtT1y5hH",
        "outputId": "d74c7bc2-9aa0-4194-f898-b095e4a72193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LetterC\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_0_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_0_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_1_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_1_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_2_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_2_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_3_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_3_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_4_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_4_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_5_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_5_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_6_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_6_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_7_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_7_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_8_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_8_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_9_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_9_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_10_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_10_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_11_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_11_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_12_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_12_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_13_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_13_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_14_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_14_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_15_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_15_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_16_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_16_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_17_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_17_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_18_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_18_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_19_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_19_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_20_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_20_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_21_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_21_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_22_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_22_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_23_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_23_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_24_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_24_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_25_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_25_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_26_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_26_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_27_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_27_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_28_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_28_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_29_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_29_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_30_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_30_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_31_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_31_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_32_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_32_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_33_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_33_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_34_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_34_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_35_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_35_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_36_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_36_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_37_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_37_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_38_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_38_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_39_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_39_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_40_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_40_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_41_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_41_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_42_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_42_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_43_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_43_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_44_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_44_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_45_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_45_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_46_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_46_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_47_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_47_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_48_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_48_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_49_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_49_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_50_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_50_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_51_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_51_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_52_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_52_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_53_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_53_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_54_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_54_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_55_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_55_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_56_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_56_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_57_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_57_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_58_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_58_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_59_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_59_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_60_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_60_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_61_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_61_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_62_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_62_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_63_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_63_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_64_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_64_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_65_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_65_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_66_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_66_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_67_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_67_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_68_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_68_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_69_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_69_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_70_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_70_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_71_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_71_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_72_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_72_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_73_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_73_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_74_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_74_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_75_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_75_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_76_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_76_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_77_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_77_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_78_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_78_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_79_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_79_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_80_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_80_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_81_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_81_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_82_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_82_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_83_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_83_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_84_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_84_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_85_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_85_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_86_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_86_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_87_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_87_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_88_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_88_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_89_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_89_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_90_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_90_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_91_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_91_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_92_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_92_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_93_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_93_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_94_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_94_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_95_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_95_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_96_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_96_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_97_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_97_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_98_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_98_224\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_99_16\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/C_99_224\n"
          ]
        }
      ],
      "source": [
        "## Test for the Picture save function \n",
        "Picture_save(storage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfFDPbQhyNjt"
      },
      "outputs": [],
      "source": [
        "# This changes the shape of the array of the 16x16 image and appens it into the text file dataset.txt\n",
        "def Resize_N_Write(ID, Letter, text, folder):\n",
        "  \n",
        "  \n",
        "  #SDataset=input(\"Choose SubDataset to save images to\")\n",
        "  path = \"/content/drive/MyDrive/ComputerVision/Data/\"\n",
        "  #flat_sixteen  = im_16.shape([1,256])\n",
        "  img = cv2.imread(path + folder + \"/\" + Letter + \"_\" + str(ID) + \"_\"+\"16.jpg\")\n",
        "  #print(path + folder + \"/\" + Letter + \"_\" + str(ID) + \"_\" + \"16.jpg\")\n",
        "  res = cv2.resize(img, dsize=(1,256), interpolation=cv2.INTER_CUBIC) # 16x16 to (1,256) 1row | 256 columns\n",
        "  flat_res = [str(a[0][0]) for a in res] # 1 with 256 elem\n",
        "  #print(flat_res)\n",
        "\n",
        "  ## write this flat_res to file \n",
        "\n",
        "  is_empty = False\n",
        "  try:\n",
        "    with  open(\"/content/gdrive/MyDrive/ComputerVision/Data/\"+ folder + '/' + text, \"r\") as fileobj: # open the dataset in read mode\n",
        "      if len(fileobj.readlines())==0: # checks if the dataset is empty\n",
        "        is_empty = True\n",
        "  except:\n",
        "    is_empty == True # if the dataset does not exist, set the empty variable to true\n",
        "    \n",
        "  with open(\"/content/drive/MyDrive/ComputerVision/Data/\"+ folder + '/'+ text, \"a+\") as fileobj: # open the dataset in append mode\n",
        "    L_list=[letter, *flat_res] # create a list with the letter as first element, followed by all the values of the pixels\n",
        "    str_list = str(L_list)\n",
        "    str_list = str_list.replace('\\'','')[1:-1] # remove the quotes and the brackets\n",
        "    if is_empty:\n",
        "      fileobj.write(str_list + '\\n')#Creates file if file doesnt exist/also appends \n",
        "      \n",
        "    elif is_empty == False:\n",
        "      fileobj.write(str_list+'\\n')#Creates file if file doesnt exist/also appends\n",
        "     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i5PJEuiwUAv",
        "outputId": "7e51cde4-ca49-496f-d3af-6bce493fa785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\n"
          ]
        }
      ],
      "source": [
        "folder = \"RSubDataset_1\"\n",
        "text = \"dataset_1.txt\"\n",
        "print(\"/content/gdrive/MyDrive/ComputerVision/Data/\"+ folder + '/'+ text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2VLdyhnw2_E"
      },
      "outputs": [],
      "source": [
        "dataset = \"dataset.txt\"\n",
        "alphabet = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
        "L_list = [\"A\", \"T\", \"C\"]\n",
        "dataset_1 = \"dataset1.txt\"\n",
        "dataset_2 = \"dataset2.txt\"\n",
        "dataset_3 = \"dataset3.txt\"\n",
        "\n",
        "n_images = 100 \n",
        "reduced_num = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV9Y1pKDqHWE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "da110944-1ed0-4b8e-92fa-fc149810bea7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-165-81bc369f9116>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    img_path = \"/content/drive/MyDrive/ComputerVision/SubDataset_\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ],
      "source": [
        "img_path = \"/content/drive/MyDrive/ComputerVision/SubDataset_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeBALlc_tuWZ",
        "outputId": "47bfa761-5852-47ba-eba9-c7cbc1375d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "range(0, 100)\n"
          ]
        }
      ],
      "source": [
        "print(n_images)\n",
        "print(range(n_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XctJDBVbBYhK"
      },
      "outputs": [],
      "source": [
        "# reads the images and then converts to (1,256) and then appends them to a text file\n",
        "for q in range(n_images):\n",
        "  for letter in L_list:\n",
        "    Resize_N_Write(q, letter, 'dataset_1.txt', 'RSubDataset_1') # change this to match the subset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYLjnF16GbVW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGcgXNlXZ-ft"
      },
      "outputs": [],
      "source": [
        "for i in range(reduced_num):\n",
        "  for letter in L_list[0]:\n",
        "    Resize_N_Write(i, letter, \"dataset_2.txt\", 'RSubDataset_2')\n",
        "for q in range(n_images):\n",
        "  for letter in L_list[1:]:\n",
        "    Resize_N_Write(q, letter, \"dataset_2.txt\", 'RSubDataset_2') # change this to match the subset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF-HPwL70gi4"
      },
      "outputs": [],
      "source": [
        "for q in range(n_images):\n",
        "  for letter in L_list:\n",
        "    Resize_N_Write(q, letter, \"dataset_3.txt\", 'RSubDataset_3') # change this to match the subset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pezSRug4TCF"
      },
      "outputs": [],
      "source": [
        "import random #this doesn't change anything with or without new line breaks or inside brackets\n",
        "data = [1, 2 ,3]\n",
        "for d in data:\n",
        "  lines = open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_\"+str(d)+\"/dataset_\"+str(d)+\".txt\").readlines()\n",
        "  random.shuffle(lines)\n",
        "  open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_\"+str(d)+\"/dataset_\"+str(d)+\"r.txt\", 'w').writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4YA81UvrUG2"
      },
      "outputs": [],
      "source": [
        "lines = open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\").readlines()\n",
        "random.shuffle(lines)\n",
        "open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\", 'w').writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bv2NSr-Akji"
      },
      "outputs": [],
      "source": [
        "lines = open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\").readlines()\n",
        "random.shuffle(lines)\n",
        "open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\", 'w').writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j4m8BdAuHg4"
      },
      "outputs": [],
      "source": [
        "lines = open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\").readlines()\n",
        "random.shuffle(lines)\n",
        "open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\", 'w').writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7IL4sV-nnfO"
      },
      "outputs": [],
      "source": [
        "dataset_file_path1 = \"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\"\n",
        "dataset_file_path2 = \"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\"\n",
        "dataset_file_path3 = \"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/dataset_3.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lyl-4fen_EB"
      },
      "outputs": [],
      "source": [
        "#The following function takes the dataset file and returns two arrays: samples and letters.\n",
        "def load_dataset(dataset_file_path):\n",
        "    a = np.loadtxt(dataset_file_path, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })\n",
        "    samples, letters = a[:,1:], a[:,0] # samples takes all the rows and all the columns except the first column\n",
        "    # letters: keep only the first column\n",
        "    return samples, letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOuI4oUDoEcT",
        "outputId": "3ab4c3aa-bcaa-4b7a-a2d2-727ea06d8323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 256)\n"
          ]
        }
      ],
      "source": [
        "#here we split the dataset for training and validation\n",
        "train_ratio = 0.7\n",
        "samples, letters = load_dataset(dataset_file_path1)\n",
        "n_train_samples = int(len(samples) * train_ratio)\n",
        "x_train, y_train = samples[:n_train_samples], letters[:n_train_samples] # keeps only the first 70% of rows\n",
        "x_val, y_val = samples[n_train_samples:], letters[n_train_samples:] # keeps only the last 30%\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVJjF7eI1Gi"
      },
      "source": [
        "# Training the models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkhumvTrCPnC",
        "outputId": "4ac3b1f1-82dd-4213-cf91-497f04c42f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210 train samples\n",
            "90 test samples\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_287 (Dense)           (None, 26)                6682      \n",
            "                                                                 \n",
            " dense_288 (Dense)           (None, 52)                1404      \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 52)                0         \n",
            "                                                                 \n",
            " dense_289 (Dense)           (None, 208)               11024     \n",
            "                                                                 \n",
            " dense_290 (Dense)           (None, 6656)              1391104   \n",
            "                                                                 \n",
            " dense_291 (Dense)           (None, 13)                86541     \n",
            "                                                                 \n",
            " dense_292 (Dense)           (None, 6)                 84        \n",
            "                                                                 \n",
            " dense_293 (Dense)           (None, 26)                182       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,497,021\n",
            "Trainable params: 1,497,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "7/7 [==============================] - 1s 52ms/step - loss: 0.1508 - accuracy: 0.3286 - val_loss: 0.1417 - val_accuracy: 0.3333\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1434 - accuracy: 0.3333 - val_loss: 0.1456 - val_accuracy: 0.3333\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1383 - accuracy: 0.3333 - val_loss: 0.1383 - val_accuracy: 0.3333\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1353 - accuracy: 0.3333 - val_loss: 0.1367 - val_accuracy: 0.3333\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1319 - accuracy: 0.3333 - val_loss: 0.1342 - val_accuracy: 0.3333\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1276 - accuracy: 0.3333 - val_loss: 0.1296 - val_accuracy: 0.3333\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1253 - accuracy: 0.3333 - val_loss: 0.1286 - val_accuracy: 0.3333\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.1223 - accuracy: 0.3333 - val_loss: 0.1229 - val_accuracy: 0.3333\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1211 - accuracy: 0.3333 - val_loss: 0.1241 - val_accuracy: 0.3333\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.1177 - accuracy: 0.3333 - val_loss: 0.1194 - val_accuracy: 0.3333\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1143 - accuracy: 0.3333 - val_loss: 0.1164 - val_accuracy: 0.3333\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1115 - accuracy: 0.3333 - val_loss: 0.1155 - val_accuracy: 0.3333\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1086 - accuracy: 0.3333 - val_loss: 0.1104 - val_accuracy: 0.3333\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.1059 - accuracy: 0.3333 - val_loss: 0.1090 - val_accuracy: 0.3333\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1025 - accuracy: 0.3333 - val_loss: 0.1041 - val_accuracy: 0.3333\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0976 - accuracy: 0.3333 - val_loss: 0.1003 - val_accuracy: 0.3333\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0917 - accuracy: 0.3476 - val_loss: 0.0953 - val_accuracy: 0.3111\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0881 - accuracy: 0.4286 - val_loss: 0.0910 - val_accuracy: 0.4111\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0802 - accuracy: 0.5238 - val_loss: 0.0864 - val_accuracy: 0.3667\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0759 - accuracy: 0.5048 - val_loss: 0.0838 - val_accuracy: 0.4111\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0746 - accuracy: 0.5524 - val_loss: 0.0816 - val_accuracy: 0.4000\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0746 - accuracy: 0.4857 - val_loss: 0.0812 - val_accuracy: 0.5333\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0707 - accuracy: 0.5714 - val_loss: 0.0797 - val_accuracy: 0.4667\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0695 - accuracy: 0.5476 - val_loss: 0.0795 - val_accuracy: 0.4000\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0700 - accuracy: 0.5476 - val_loss: 0.0788 - val_accuracy: 0.4000\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0689 - accuracy: 0.5524 - val_loss: 0.0788 - val_accuracy: 0.3889\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0693 - accuracy: 0.5476 - val_loss: 0.0766 - val_accuracy: 0.4778\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0687 - accuracy: 0.5571 - val_loss: 0.0767 - val_accuracy: 0.4000\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0688 - accuracy: 0.5571 - val_loss: 0.0766 - val_accuracy: 0.4111\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0670 - accuracy: 0.5476 - val_loss: 0.0744 - val_accuracy: 0.6222\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0680 - accuracy: 0.6143 - val_loss: 0.0751 - val_accuracy: 0.6667\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0687 - accuracy: 0.5714 - val_loss: 0.0768 - val_accuracy: 0.3889\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0671 - accuracy: 0.6095 - val_loss: 0.0760 - val_accuracy: 0.6333\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0672 - accuracy: 0.5810 - val_loss: 0.0763 - val_accuracy: 0.5778\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0668 - accuracy: 0.5857 - val_loss: 0.0757 - val_accuracy: 0.6222\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0656 - accuracy: 0.6238 - val_loss: 0.0758 - val_accuracy: 0.6333\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0655 - accuracy: 0.7048 - val_loss: 0.0757 - val_accuracy: 0.6556\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0684 - accuracy: 0.6429 - val_loss: 0.0740 - val_accuracy: 0.6222\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0666 - accuracy: 0.6286 - val_loss: 0.0718 - val_accuracy: 0.6889\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0666 - accuracy: 0.6381 - val_loss: 0.0709 - val_accuracy: 0.6889\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0648 - accuracy: 0.6571 - val_loss: 0.0702 - val_accuracy: 0.7222\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0653 - accuracy: 0.7095 - val_loss: 0.0707 - val_accuracy: 0.7000\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0638 - accuracy: 0.7000 - val_loss: 0.0722 - val_accuracy: 0.6889\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0637 - accuracy: 0.7238 - val_loss: 0.0715 - val_accuracy: 0.6556\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0633 - accuracy: 0.7619 - val_loss: 0.0693 - val_accuracy: 0.7222\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0633 - accuracy: 0.7048 - val_loss: 0.0688 - val_accuracy: 0.7000\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0639 - accuracy: 0.6905 - val_loss: 0.0678 - val_accuracy: 0.7111\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0639 - accuracy: 0.7143 - val_loss: 0.0680 - val_accuracy: 0.7222\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0642 - accuracy: 0.6905 - val_loss: 0.0686 - val_accuracy: 0.6889\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0651 - accuracy: 0.7381 - val_loss: 0.0686 - val_accuracy: 0.6778\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0628 - accuracy: 0.7476 - val_loss: 0.0686 - val_accuracy: 0.7222\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0620 - accuracy: 0.7667 - val_loss: 0.0694 - val_accuracy: 0.7333\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0631 - accuracy: 0.7333 - val_loss: 0.0684 - val_accuracy: 0.7333\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0633 - accuracy: 0.7286 - val_loss: 0.0667 - val_accuracy: 0.7444\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0617 - accuracy: 0.7476 - val_loss: 0.0674 - val_accuracy: 0.7444\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0624 - accuracy: 0.7381 - val_loss: 0.0708 - val_accuracy: 0.6667\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0621 - accuracy: 0.7429 - val_loss: 0.0673 - val_accuracy: 0.7556\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0601 - accuracy: 0.7476 - val_loss: 0.0662 - val_accuracy: 0.7556\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0616 - accuracy: 0.7429 - val_loss: 0.0663 - val_accuracy: 0.7333\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0622 - accuracy: 0.7333 - val_loss: 0.0680 - val_accuracy: 0.6889\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0601 - accuracy: 0.7714 - val_loss: 0.0651 - val_accuracy: 0.7556\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0605 - accuracy: 0.7667 - val_loss: 0.0661 - val_accuracy: 0.7333\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0600 - accuracy: 0.7571 - val_loss: 0.0662 - val_accuracy: 0.7556\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0597 - accuracy: 0.7714 - val_loss: 0.0651 - val_accuracy: 0.7667\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0606 - accuracy: 0.7667 - val_loss: 0.0686 - val_accuracy: 0.6889\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0587 - accuracy: 0.7714 - val_loss: 0.0645 - val_accuracy: 0.7556\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0605 - accuracy: 0.7762 - val_loss: 0.0682 - val_accuracy: 0.6778\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0583 - accuracy: 0.7762 - val_loss: 0.0686 - val_accuracy: 0.6889\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0609 - accuracy: 0.7810 - val_loss: 0.0655 - val_accuracy: 0.7778\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0587 - accuracy: 0.7810 - val_loss: 0.0684 - val_accuracy: 0.6889\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0588 - accuracy: 0.7810 - val_loss: 0.0649 - val_accuracy: 0.7556\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0600 - accuracy: 0.7810 - val_loss: 0.0634 - val_accuracy: 0.7778\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0587 - accuracy: 0.8143 - val_loss: 0.0639 - val_accuracy: 0.7667\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0595 - accuracy: 0.7810 - val_loss: 0.0667 - val_accuracy: 0.7222\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0582 - accuracy: 0.7667 - val_loss: 0.0672 - val_accuracy: 0.7000\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0591 - accuracy: 0.7667 - val_loss: 0.0675 - val_accuracy: 0.7000\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0592 - accuracy: 0.7429 - val_loss: 0.0682 - val_accuracy: 0.7111\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0581 - accuracy: 0.7714 - val_loss: 0.0669 - val_accuracy: 0.7444\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0579 - accuracy: 0.7905 - val_loss: 0.0687 - val_accuracy: 0.6889\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0590 - accuracy: 0.7952 - val_loss: 0.0666 - val_accuracy: 0.7111\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0562 - accuracy: 0.7952 - val_loss: 0.0670 - val_accuracy: 0.7222\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0574 - accuracy: 0.7905 - val_loss: 0.0684 - val_accuracy: 0.7000\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0567 - accuracy: 0.7905 - val_loss: 0.0671 - val_accuracy: 0.7333\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0578 - accuracy: 0.7952 - val_loss: 0.0648 - val_accuracy: 0.7333\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0587 - accuracy: 0.7857 - val_loss: 0.0701 - val_accuracy: 0.6556\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0572 - accuracy: 0.8238 - val_loss: 0.0668 - val_accuracy: 0.7556\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0564 - accuracy: 0.8048 - val_loss: 0.0686 - val_accuracy: 0.7000\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0576 - accuracy: 0.8143 - val_loss: 0.0733 - val_accuracy: 0.6222\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0556 - accuracy: 0.8238 - val_loss: 0.0666 - val_accuracy: 0.7333\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0551 - accuracy: 0.8238 - val_loss: 0.0677 - val_accuracy: 0.7333\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0558 - accuracy: 0.8333 - val_loss: 0.0715 - val_accuracy: 0.6444\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0563 - accuracy: 0.8048 - val_loss: 0.0686 - val_accuracy: 0.6778\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0556 - accuracy: 0.8476 - val_loss: 0.0723 - val_accuracy: 0.6111\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0560 - accuracy: 0.8429 - val_loss: 0.0691 - val_accuracy: 0.7111\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0548 - accuracy: 0.8476 - val_loss: 0.0668 - val_accuracy: 0.7444\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0563 - accuracy: 0.8190 - val_loss: 0.0695 - val_accuracy: 0.6889\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0553 - accuracy: 0.8333 - val_loss: 0.0755 - val_accuracy: 0.6333\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0547 - accuracy: 0.8381 - val_loss: 0.0711 - val_accuracy: 0.6778\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0547 - accuracy: 0.8333 - val_loss: 0.0711 - val_accuracy: 0.6556\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0555 - accuracy: 0.8143 - val_loss: 0.0683 - val_accuracy: 0.7111\n",
            "Validation loss: 0.068294458091259\n",
            "Validation accuracy: 0.7111111283302307\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "\n",
        "num_classes = 26 # number of letters\n",
        "epochs = 100 # number of training s\n",
        "\n",
        "\n",
        "#here we split the dataset for training and validation\n",
        "train_ratio = 0.7\n",
        "samples, letters = load_dataset(dataset_file_path3)\n",
        "n_train_samples = int(len(samples) * train_ratio)\n",
        "x_train, y_train = samples[:n_train_samples], letters[:n_train_samples] # keeps only the first 70% of rows\n",
        "x_val, y_val = samples[n_train_samples:], letters[n_train_samples:] # keeps only the last 30%\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "x_train /= 255 # x_train=x_train/255    normalize the numbers in the interval [0,1]\n",
        "x_val /= 255\n",
        "print(x_train.shape[0], 'train samples') # print the number of images\n",
        "print(x_val.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes) # converts the prediction letter into a number encoding for that letter\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential() \n",
        "tf.keras.layers.RandomFlip(\"horizontal\") # applied augmentation however, some despite increasing the num of epochs reduce val accuracy\n",
        "model.add(Dense(26, activation='relu', input_shape=(256,))) # add a relu layer\n",
        "model.add(Dense(52, activation='relu')) # add another relu\n",
        "model.add(Dropout(0.7)) # add a regularizator, This helps to reduce the validation over time(could affect accuracy)\n",
        "model.add(Dense(208, activation='relu')) # add anothe relu\n",
        "model.add(Dense(6656, activation='relu')) # add another relu  # Created a layer with thousands of neuron and to then pass into a smaller layer \n",
        "model.add(Dense(13, activation='relu')) # add anotherr relu # this smaller helps to compress and learn more complex features. \n",
        "#model.add(Dropout(0.2)) # add a regularizator\n",
        "model.add(Dense(6, activation='relu')) # add anothe relu # Another compression into the output\n",
        "model.add(Dense(num_classes, activation='softmax'))  # add a final softmax layer\n",
        "\n",
        "#Increased the complexity of the model to improve the accuracy as the poisson loss was relatively low. \n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Added a learning that slowly decays over time to help improve finding a minimum\n",
        "# Need to make sure it is not to small so that the updates are able to change the accuracy\n",
        "# but not pass by the global min.\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=.001, # The best value was a low learning that decreased overtime\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr_schedule )\n",
        "\n",
        "model.compile(loss='poisson',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "# Used poisson loss adam optimizer \n",
        "\n",
        "history = model.fit(x_train, y_train,   \n",
        "                    epochs=epochs,\n",
        "                    verbose=1, # if verbose=1 print data for each epoch\n",
        "                    validation_data=(x_val, y_val)) # train the network\n",
        "score = model.evaluate(x_val, y_val, verbose=0)  #model and dataset\n",
        "print('Validation loss:', score[0]) # print the loss\n",
        "print('Validation accuracy:', score[1]) # print the accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8utk6ySJCvu"
      },
      "source": [
        "#Save model to Disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHyKKgRtI-XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9adccc9-3f8a-4ebb-eecc-7c4a33142719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/model3.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/model3_weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_iN58PLJJpP"
      },
      "source": [
        "# Evaluation of the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOiHkz8lJIvB"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(path, model):\n",
        "  train_ratio = 0.7\n",
        "  samples, letters = load_dataset(path) # load the dataset\n",
        "  n_train_samples = int(len(samples) * train_ratio)\n",
        "  x_train, y_train = samples[:n_train_samples], letters[:n_train_samples] # keeps only the first 70% of rows\n",
        "  x_val, y_val = samples[n_train_samples:], letters[n_train_samples:] # keeps only the last 30%\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_val = x_val.astype('float32')\n",
        "  x_train /= 255 # x_train=x_train/255    normalize the numbers in the interval [0,1]\n",
        "  x_val /= 255\n",
        "  \n",
        "  print(x_train.shape[0], 'train samples') # print the number of images\n",
        "  print(x_val.shape[0], 'test samples')\n",
        "\n",
        "  # convert class vectors to binary class matrices\n",
        "  y_train = tf.keras.utils.to_categorical(y_train, num_classes) # converts the prediction letter into a number encoding for that letter\n",
        "  y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "\n",
        "  score = model.evaluate(x_val, y_val, verbose=0) # valuta il modello ottenuto sul validation set.  #model and dataset\n",
        "  print('Validation loss:', score[0]) # print the loss\n",
        "  print('Validation accuracy:', score[1]) # print the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeGAIW33JT5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b00f77-df67-4c7d-a928-54cd816cc346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_273 (Dense)           (None, 26)                6682      \n",
            "                                                                 \n",
            " dense_274 (Dense)           (None, 52)                1404      \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 52)                0         \n",
            "                                                                 \n",
            " dense_275 (Dense)           (None, 208)               11024     \n",
            "                                                                 \n",
            " dense_276 (Dense)           (None, 6656)              1391104   \n",
            "                                                                 \n",
            " dense_277 (Dense)           (None, 13)                86541     \n",
            "                                                                 \n",
            " dense_278 (Dense)           (None, 6)                 84        \n",
            "                                                                 \n",
            " dense_279 (Dense)           (None, 26)                182       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,497,021\n",
            "Trainable params: 1,497,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\n",
            "210 train samples\n",
            "90 test samples\n",
            "Validation loss: 0.08226645737886429\n",
            "Validation accuracy: 0.6000000238418579\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\n",
            "175 train samples\n",
            "75 test samples\n",
            "Validation loss: 0.06505387276411057\n",
            "Validation accuracy: 0.7866666913032532\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/dataset_3.txt\n",
            "210 train samples\n",
            "90 test samples\n",
            "Validation loss: 0.10933221131563187\n",
            "Validation accuracy: 0.36666667461395264\n"
          ]
        }
      ],
      "source": [
        "from keras.models import model_from_json\n",
        "g_path = \"/content/drive/MyDrive/ComputerVision/Data/\"\n",
        "# load json and create model\n",
        "json_file = open(g_path+'RSubDataset_1/model1.json', 'r') \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "loaded_model.summary()\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(g_path+\"RSubDataset_1/model1_weights.h5\")\n",
        "\n",
        "loaded_model.compile(loss='poisson',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dataset_file_paths = [g_path+\"RSubDataset_1/dataset_1.txt\", g_path+\"RSubDataset_2/dataset_2.txt\", g_path+\"RSubDataset_3/dataset_3.txt\"]\n",
        "for path in dataset_file_paths:\n",
        "  print(path)\n",
        "  evaluate_model(path, loaded_model)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "g_path = \"/content/drive/MyDrive/ComputerVision/Data/\"\n",
        "# load json and create model\n",
        "json_file = open(g_path+'RSubDataset_2/model2.json', 'r') \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model2 = model_from_json(loaded_model_json)\n",
        "\n",
        "loaded_model2.summary()\n",
        "# load weights into new model\n",
        "loaded_model2.load_weights(g_path+\"RSubDataset_2/model2_weights.h5\")\n",
        "\n",
        "loaded_model2.compile(loss='poisson',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dataset_file_paths = [g_path+\"RSubDataset_1/dataset_1.txt\", g_path+\"RSubDataset_2/dataset_2.txt\", g_path+\"RSubDataset_3/dataset_3.txt\"]\n",
        "for path in dataset_file_paths:\n",
        "  print(path)\n",
        "  evaluate_model(path, loaded_model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxRefrwKK6lX",
        "outputId": "2347d9bc-35ee-4fc4-ce1f-c21c66873e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_280 (Dense)           (None, 26)                6682      \n",
            "                                                                 \n",
            " dense_281 (Dense)           (None, 52)                1404      \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 52)                0         \n",
            "                                                                 \n",
            " dense_282 (Dense)           (None, 208)               11024     \n",
            "                                                                 \n",
            " dense_283 (Dense)           (None, 6656)              1391104   \n",
            "                                                                 \n",
            " dense_284 (Dense)           (None, 13)                86541     \n",
            "                                                                 \n",
            " dense_285 (Dense)           (None, 6)                 84        \n",
            "                                                                 \n",
            " dense_286 (Dense)           (None, 26)                182       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,497,021\n",
            "Trainable params: 1,497,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\n",
            "210 train samples\n",
            "90 test samples\n",
            "Validation loss: 0.0727599486708641\n",
            "Validation accuracy: 0.644444465637207\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\n",
            "175 train samples\n",
            "75 test samples\n",
            "Validation loss: 0.0986844003200531\n",
            "Validation accuracy: 0.5733333230018616\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/dataset_3.txt\n",
            "210 train samples\n",
            "90 test samples\n",
            "Validation loss: 0.11517108231782913\n",
            "Validation accuracy: 0.31111112236976624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "g_path = \"/content/drive/MyDrive/ComputerVision/Data/\"\n",
        "# load json and create model\n",
        "json_file = open(g_path+'RSubDataset_3/model3.json', 'r') \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model3 = model_from_json(loaded_model_json)\n",
        "\n",
        "loaded_model3.summary()\n",
        "# load weights into new model\n",
        "loaded_model3.load_weights(g_path+\"RSubDataset_3/model3_weights.h5\")\n",
        "\n",
        "loaded_model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dataset_file_paths = [g_path+\"RSubDataset_1/dataset_1.txt\", g_path+\"RSubDataset_2/dataset_2.txt\", g_path+\"RSubDataset_3/dataset_3.txt\"]\n",
        "for path in dataset_file_paths:\n",
        "  print(path)\n",
        "  evaluate_model(path, loaded_model3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luTapFVFK81B",
        "outputId": "4844212f-a141-470f-e413-02c637ea3d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_287 (Dense)           (None, 26)                6682      \n",
            "                                                                 \n",
            " dense_288 (Dense)           (None, 52)                1404      \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 52)                0         \n",
            "                                                                 \n",
            " dense_289 (Dense)           (None, 208)               11024     \n",
            "                                                                 \n",
            " dense_290 (Dense)           (None, 6656)              1391104   \n",
            "                                                                 \n",
            " dense_291 (Dense)           (None, 13)                86541     \n",
            "                                                                 \n",
            " dense_292 (Dense)           (None, 6)                 84        \n",
            "                                                                 \n",
            " dense_293 (Dense)           (None, 26)                182       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,497,021\n",
            "Trainable params: 1,497,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_1/dataset_1.txt\n",
            "210 train samples\n",
            "90 test samples\n",
            "Validation loss: 3.0160210132598877\n",
            "Validation accuracy: 0.4000000059604645\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_2/dataset_2.txt\n",
            "175 train samples\n",
            "75 test samples\n",
            "Validation loss: 2.5702521800994873\n",
            "Validation accuracy: 0.3866666555404663\n",
            "/content/drive/MyDrive/ComputerVision/Data/RSubDataset_3/dataset_3.txt\n",
            "210 train samples\n",
            "90 test samples\n",
            "Validation loss: 0.7756565809249878\n",
            "Validation accuracy: 0.7111111283302307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#A T C\n",
        "\n",
        "VideoCapture() \n",
        "eval_js('create()') \n",
        "\n",
        "Cascade_visage = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "FDetect=False\n",
        "margin = 30\n",
        "restr = 30\n",
        "\n",
        "x=0\n",
        "picture_store =[]\n",
        "\n",
        "\n",
        "while not FDetect:\n",
        "  bigrect=[] \n",
        "  byte = eval_js('capture()')\n",
        "  #norm_rgb = normalized(byte)\n",
        "  im = byte2image(byte)\n",
        "  #norm_rgb = normalized(im)\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
        "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
        "  face = detect(gray, cascade = Cascade_visage) # face contains the rectangle\n",
        "  \n",
        "  if len(face)==0:\n",
        "    FDetect=False\n",
        "  else:\n",
        "    FDetect=True\n",
        "\n",
        "    \n",
        "  if FDetect: # if a face is found\n",
        "\n",
        "  \n",
        "    r_face = rest(face[0])\n",
        "    hsv_roi=hsv[r_face[1]:r_face[3], r_face[0]:r_face[2]] # !!! better to restrict this face for the histogram\n",
        "    mask_roi = cv2.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "    kernel= cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,1))\n",
        "    mask_roi = cv2.morphologyEx(mask_roi, cv2.MORPH_CLOSE, kernel) #removes unnecessary black noises from the white region.\n",
        "    #Closing is reverse of Opening, Dilation followed by Erosion.\n",
        "    mask_roi = cv2.morphologyEx(mask_roi, cv2.MORPH_OPEN, kernel) #removes white noise from the black region of the mask.\n",
        "    #Opening is just another name of erosion followed by dilation.\n",
        "    bigrect.append(compute_coordinates(face[0])) # compute bigrect and add to the list\n",
        "\n",
        "    Detection=True # set a flag to start camshift\n",
        "\n",
        "\n",
        "    hist=cv2.calcHist([hsv_roi], [0], mask_roi, [180], [0,180]) # compute the histogram\n",
        "\n",
        "    # Region of interest identified\n",
        "    \n",
        "    hist=cv2.normalize(hist, hist,  alpha=0, beta=255, norm_type=cv2.NORM_MINMAX) # normalize the histogram \n",
        "\n",
        "  \n",
        "    track_window = (face[0][0], face[0][1], face[0][2], face[0][3])\n",
        "   # sets some criteria to iterate 10 times\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 ) ##\n",
        "\n",
        "\n",
        "while Detection:\n",
        "\n",
        "   \n",
        "    #term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 ) ##\n",
        "\n",
        "    byte = eval_js('capture()')\n",
        "    im = byte2image(byte)\n",
        "    #im = cv2.normalize(im, gray, 0, 180, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC3)\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
        "    hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #\n",
        "    mask = cv2.inRange(hsv, np.array((0,64,32)), np.array((180,200,200)))\n",
        "    # (0,64,32) ( 180, 200,200)\n",
        "      # compute the region of interest (bigrect)\n",
        "    raw_im = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
        "    #im = cv2.normalize(im, hsv, 0, 180, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC3)\n",
        "\n",
        "    #prob = cv2.calcBackProject([hsv], [0], hist, [0, 255], 1) # compute the probability matrix \n",
        "    prob = cv2.calcBackProject([hsv], [0], hist, [0, 180], 1) \n",
        "\n",
        "      \n",
        "\n",
        "      # perform camshift over the track_window\n",
        "\n",
        "      \n",
        "    \n",
        "    for i in range(bigrect[0][0], bigrect[0][2]): # iteration  between coordinates\n",
        "      for j in range(bigrect[0][1], bigrect[0][3]): \n",
        "        prob[j,i]=0 # sets probability matrix to 0 on all points inside bigrect\n",
        "\n",
        "    prob &= mask\n",
        "\n",
        "    track_box, track_window = cv2.CamShift(prob, track_window, term_crit)\n",
        "\n",
        "    im[:] = prob[...,np.newaxis] # add probability matrix to the image\n",
        "\n",
        "\n",
        "    #eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
        "    \n",
        "\n",
        "    cropped_image = im[int(track_window[1]):int(track_window[1]+track_window[3]), int(track_window[0]):int(track_window[0]+track_window[2])]\n",
        "\n",
        "\n",
        "# Implementing the model on the vidoe\n",
        "    resized_img = cv2.resize(cropped_image, (16, 16), interpolation = cv2.INTER_AREA)\n",
        "    resized_img = cv2.resize(resized_img, dsize=(1,256), interpolation=cv2.INTER_CUBIC)\n",
        "    resized_img = np.array([str(a[0][0]) for a in resized_img])\n",
        "    resized_img=resized_img.astype('float32')\n",
        "    resized_img /= 255.0\n",
        "    prediction = loaded_model3.predict(resized_img[np.newaxis, :]) # where hand_image is the probability image of your hand of size (1,256)\n",
        "    prediction = prediction.argmax()\n",
        "    predicted_letter = chr(ord('A') + prediction)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(im, predicted_letter, (int(track_window[0]), int(track_window[1])), font, 1, (0, 255, 0), 1, cv2.LINE_4)\n",
        "\n",
        "    eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
        "    x+=1\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Oo12Pqt1Ijxf",
        "outputId": "2d3aef02-66ea-453c-a1ed-67d5edf43db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-248-1e75d3116748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_window\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_window\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINE_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: Cell has no view"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Models 1, 2, and 3\n",
        "\n",
        "\n",
        "\n",
        "Dataset 1: \n",
        "The first dataset looks at three letters with high variability. This dataset emphasizes how variability impacts the training of the model. The model itself performed worse on dataset 2 and 3, which is due to the high variability. For the model to perform better more data would be needed and a longer training time is required. However, with the increase in data and the longer training time the increased variability would in fact make the model more robust when applied to the real world. \n",
        "\n",
        "Dataset 2: \n",
        "The second dataset focuses on three letters with varying amounts of images to train the model. This leads the model to make increasingly inaccurate predictions on certain classes as it is unable to differentiate differences between spatial aspects of the pixels. This reduces accuracy for some classes and makes the model less robust for certain hand gestures. \n",
        "\n",
        "Dataset 3: \n",
        "The third dataset has the highest loss due to its lack of variability. It focuses on three letters with low variability. Looking at the accuracy it has, it is worse than the other two models as it cannot generalize any variability in the pictures. Meaning, if the hand is tilted if you use the opposite hand, it won't be able to accurately predict the gesture.\n",
        "\n",
        "The Model:\n",
        "The model I developed consists of 5 relu hidden layers with varying number of neurons ,input relu layer with 26 neurons, and an output softmax layer. There is one dropout layer with a probability set to .7. Overall, there are 1,497,021 parameters, this was developed in an attempt to improve the validation accuracy, over 100 epochs. To help avoid overfitting and to increase the data size an augmentation was added to horizontal flip the image or the image matrix. Dropout was placed closer before the layer with more drastic increases in the number of neuron because it helps to reduce validation loss and improve generalization, without impacting more complex features. I implemented a compression from a dense layer of 6656 to 13  to compress more basic features learning into more complex ones to improve accuracy. With other parameters I tested them out multiple times with specific values. In regards to the learning rate, i set up a learning rate scheduler that exponentially decays the initial learning rate so that it is able to find an optimum minimum without resulting in divergence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GOy36AzqTI_Q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
